{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1754fa4b-e8fd-425b-bc92-eebbbf58ca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fsspec==2023.10.0 in /home/cv/.local/lib/python3.9/site-packages (2023.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fsspec==2023.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303314f6-a748-4a57-831e-376d208044ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa1844a-a6fe-4de7-91fc-23504fb50b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q datasets bitsandbytes einops wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2329e704-d49a-4864-8d86-6e6ce8d3d51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/cv/.local/lib/python3.9/site-packages (24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d6a33e-9583-4c91-b3fb-51850a9b786b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60f6dc92aac490696f1194325ceabf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/395 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cv/.local/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc394e17b2a474f9f1b2aaff17e20f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f144d5204d4458bad78ce732feaf39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db875ce8625e4dcbb638f82fb5f3611f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a0e7d6923e442f97dacb023abfca17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/518 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the necessary library for loading datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Specify the name of the dataset\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "\n",
    "# Load the dataset from the specified name and select the \"train\" split\n",
    "dataset = load_dataset(dataset_name, split=\"train\", download_mode=\"force_redownload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edcd58e2-ec15-4eeb-b081-1a1634971e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79b2453a3c14f81ac05abacc3acf715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Defining the name of the Falcon model\n",
    "model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
    "\n",
    "# Configuring the BitsAndBytes quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "load_in_4bit=True,\n",
    "bnb_4bit_quant_type=\"nf4\",\n",
    "bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Loading the Falcon model with quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "model_name,\n",
    "quantization_config=bnb_config,\n",
    "trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Disabling cache usage in the model configuration\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bca9699-3845-4924-a710-d9a30c8c8889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for the Falcon 7B model with remote code trust\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Set the padding token to be the same as the end-of-sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "544aec53-5fde-4b9e-ae1f-c882eb9c51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary module for LoRA configuration\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Define the parameters for LoRA configuration\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "\n",
    "# Create the LoRA configuration object\n",
    "peft_config = LoraConfig(\n",
    "lora_alpha=lora_alpha,\n",
    "lora_dropout=lora_dropout,\n",
    "r=lora_r,\n",
    "bias=\"none\",\n",
    "task_type=\"CAUSAL_LM\",\n",
    "target_modules=[\n",
    "\"query_key_value\",\n",
    "\"dense\",\n",
    "\"dense_h_to_4h\",\n",
    "\"dense_4h_to_h\",\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f7a3730-e0a5-4619-b32e-b137fdaafcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "# Define the directory to save training results\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Set the batch size per device during training\n",
    "per_device_train_batch_size = 2\n",
    "\n",
    "# Number of steps to accumulate gradients before updating the model\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Choose the optimizer type (e.g., \"paged_adamw_32bit\")\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Interval to save model checkpoints (every 10 steps)\n",
    "save_steps = 10\n",
    "\n",
    "# Interval to log training metrics (every 10 steps)\n",
    "logging_steps = 10\n",
    "\n",
    "# Learning rate for optimization\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Maximum gradient norm for gradient clipping\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Maximum number of training steps\n",
    "max_steps = 50\n",
    "\n",
    "# Warmup ratio for learning rate scheduling\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Type of learning rate scheduler (e.g., \"constant\")\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Create a TrainingArguments object to configure the training process\n",
    "training_arguments = TrainingArguments(\n",
    "output_dir=output_dir,\n",
    "per_device_train_batch_size=per_device_train_batch_size,\n",
    "gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "optim=optim,\n",
    "save_steps=save_steps,\n",
    "logging_steps=logging_steps,\n",
    "learning_rate=learning_rate,\n",
    "fp16=True,  # Use mixed precision training (16-bit)\n",
    "max_grad_norm=max_grad_norm,\n",
    "max_steps=max_steps,\n",
    "warmup_ratio=warmup_ratio,\n",
    "group_by_length=True,\n",
    "lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87cba87e-a67b-4a77-b1da-8db0da4db5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4180e7ab03ef4d3fb0f2e5142cc15c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Import the SFTTrainer from the TRL library\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Set the maximum sequence length\n",
    "max_seq_length = 200\n",
    "\n",
    "# Create a trainer instance using SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "model=model,\n",
    "train_dataset=dataset,\n",
    "peft_config=peft_config,\n",
    "dataset_text_field=\"text\",\n",
    "max_seq_length=max_seq_length,\n",
    "tokenizer=tokenizer,\n",
    "args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "032c65f3-a07f-463d-a23a-c49e994bf61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cd7e571-c64e-45f5-86ce-e1193ca40ea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   5236 MiB |   5300 MiB |  19032 MiB |  13796 MiB |\n",
      "|       from large pool |   5219 MiB |   5283 MiB |  19015 MiB |  13796 MiB |\n",
      "|       from small pool |     17 MiB |     17 MiB |     17 MiB |      0 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   5236 MiB |   5300 MiB |  19032 MiB |  13796 MiB |\n",
      "|       from large pool |   5219 MiB |   5283 MiB |  19015 MiB |  13796 MiB |\n",
      "|       from small pool |     17 MiB |     17 MiB |     17 MiB |      0 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   5196 MiB |   5261 MiB |  18962 MiB |  13765 MiB |\n",
      "|       from large pool |   5179 MiB |   5245 MiB |  18944 MiB |  13765 MiB |\n",
      "|       from small pool |     17 MiB |     17 MiB |     17 MiB |      0 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   5342 MiB |   5342 MiB |   6064 MiB | 739328 KiB |\n",
      "|       from large pool |   5324 MiB |   5324 MiB |   6046 MiB | 739328 KiB |\n",
      "|       from small pool |     18 MiB |     18 MiB |     18 MiB |      0 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 108220 KiB | 576705 KiB |   8480 MiB |   8374 MiB |\n",
      "|       from large pool | 107440 KiB | 575925 KiB |   8463 MiB |   8358 MiB |\n",
      "|       from small pool |    780 KiB |   2039 KiB |     16 MiB |     15 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     803    |     803    |     999    |     196    |\n",
      "|       from large pool |     513    |     513    |     643    |     130    |\n",
      "|       from small pool |     290    |     291    |     356    |      66    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     803    |     803    |     999    |     196    |\n",
      "|       from large pool |     513    |     513    |     643    |     130    |\n",
      "|       from small pool |     290    |     291    |     356    |      66    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      75    |      75    |      77    |       2    |\n",
      "|       from large pool |      66    |      67    |      68    |       2    |\n",
      "|       from small pool |       9    |       9    |       9    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       4    |      15    |     128    |     124    |\n",
      "|       from large pool |       1    |      13    |      86    |      85    |\n",
      "|       from small pool |       3    |       3    |      42    |      39    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c72d5da0-43e9-44b1-baf1-41be3d9d2cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohamed-omar-farooq\u001b[0m (\u001b[33momars-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cv/FYP_ChatBot/test02/src/wandb/run-20240220_193144-e59u8okc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omars-team/huggingface/runs/e59u8okc' target=\"_blank\">filigreed-firecracker-6</a></strong> to <a href='https://wandb.ai/omars-team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omars-team/huggingface' target=\"_blank\">https://wandb.ai/omars-team/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omars-team/huggingface/runs/e59u8okc' target=\"_blank\">https://wandb.ai/omars-team/huggingface/runs/e59u8okc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 10:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.553400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.529200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.455600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.475800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.807600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-10 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./results/checkpoint-20 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./results/checkpoint-30 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./results/checkpoint-40 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./results/checkpoint-50 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=1.5643066024780274, metrics={'train_runtime': 620.1459, 'train_samples_per_second': 0.645, 'train_steps_per_second': 0.081, 'total_flos': 2859666679534080.0, 'train_loss': 1.5643066024780274, 'epoch': 0.04})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate through the named modules of the trainer's model\n",
    "for name, module in trainer.model.named_modules():\n",
    "    # Check if the name contains \"norm\"\n",
    "    if \"norm\" in name:\n",
    "        # Convert the module to use torch.float32 data type\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce8e077d-e226-49a3-bca5-1dce85849e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Token:  ········\n",
      "Add token as git credential? (Y/n)  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /home/cv/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c05885d-63b1-4a1a-a537-387c44b6607b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d5eb9ec01a4634bd5486beb259d639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/522M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ad959e3aa94a2ea78fff1a6fe5af0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0493712a852463fadf0d778ff3375e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/omarfarooq908/results/commit/d3b4779ad9abcda464302916012d98fc7fa98d64', commit_message='omarfarooq908/falcon-7b-finetuned01', commit_description='', oid='d3b4779ad9abcda464302916012d98fc7fa98d64', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the directory where you want to save the fine-tuned model\n",
    "output_dir = \"./fine_tuned_model\"\n",
    "\n",
    "# Save the fine-tuned model using the save_model method\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "# Optionally, you can also upload the model to the Hugging Face model hub\n",
    "# if you want to share it with others\n",
    "trainer.push_to_hub(\"omarfarooq908/falcon-7b-finetuned01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6b4b2c1-266d-41f6-860a-a1393596d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import move_repo\n",
    "move_repo(from_id=\"omarfarooq908/results\", to_id=\"omarfarooq908/falcon-7b-finetuned01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ff6f84f-753a-40f4-8654-396fa63570e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save_model() got an unexpected keyword argument 'max_shard_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine_tuned_model01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model using the save_model method\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m100MB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Optionally, you can also upload the model to the Hugging Face model hub\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# if you want to share it with others\u001b[39;00m\n\u001b[1;32m      9\u001b[0m trainer\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124momarfarooq908/falcon-7b-finetuned01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: save_model() got an unexpected keyword argument 'max_shard_size'"
     ]
    }
   ],
   "source": [
    "# Define the directory where you want to save the fine-tuned model\n",
    "output_dir = \"./fine_tuned_model01-01\"\n",
    "\n",
    "# Save the fine-tuned model using the save_model method\n",
    "trainer.save_model(output_dir, max_shard_size=\"100MB\")\n",
    "\n",
    "# Optionally, you can also upload the model to the Hugging Face model hub\n",
    "# if you want to share it with others\n",
    "trainer.push_to_hub(\"omarfarooq908/falcon-7b-finetuned01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70fcc42c-e155-44f5-8652-82f118d223b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 359 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/home/cv/FYP_ChatBot/test02/src/myenv-test002-02/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b389acaa-0fb2-491b-802d-71ad2b73afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "171d48ab-3573-44e5-8e73-16896f057c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d85fe47-6be6-485a-9088-e07aac249dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_File_Path = \"./fine_tuned_model01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdae8cae-9ef2-4621-bf75-f9799efe761b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SFTTrainer' object has no attribute 'modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_File_Path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0.1GB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/accelerator.py:2562\u001b[0m, in \u001b[0;36mAccelerator.save_model\u001b[0;34m(self, model, save_directory, max_shard_size, safe_serialization)\u001b[0m\n\u001b[1;32m   2556\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_directory, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;66;03m# get the state_dict of the model\u001b[39;00m\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   2560\u001b[0m     [\n\u001b[1;32m   2561\u001b[0m         module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39moffload\n\u001b[0;32m-> 2562\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules\u001b[49m()\n\u001b[1;32m   2563\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module\u001b[38;5;241m.\u001b[39m_hf_hook, AlignDevicesHook)\n\u001b[1;32m   2564\u001b[0m     ]\n\u001b[1;32m   2565\u001b[0m ):\n\u001b[1;32m   2566\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m get_state_dict_offloaded_model(model)\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SFTTrainer' object has no attribute 'modules'"
     ]
    }
   ],
   "source": [
    "accelerator.save_model(model=trainer, save_directory=model_File_Path, max_shard_size='0.1GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a192e6-c4d0-4d4e-b608-3d6d6cfab263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-test002-02",
   "language": "python",
   "name": "myenv-test002-02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
