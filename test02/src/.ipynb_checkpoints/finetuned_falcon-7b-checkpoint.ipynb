{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd6ddab-bf93-401d-91f8-3d9a4741eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e1a346-3019-4997-838a-85d9ab10c6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3f835fff6341b0ab3e774056311235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Defining the name of the Falcon model\n",
    "model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
    "\n",
    "# Configuring the BitsAndBytes quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "load_in_4bit=True,\n",
    "bnb_4bit_quant_type=\"nf4\",\n",
    "bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Loading the Falcon model with quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "model_name,\n",
    "quantization_config=bnb_config,\n",
    "trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Disabling cache usage in the model configuration\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ff8bf4-9afb-45ce-8607-548a1f3f817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for the Falcon 7B model with remote code trust\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Set the padding token to be the same as the end-of-sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db1238c-07d0-4bfd-8415-334a617e9ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05b7f14ccfc42e680896c7edc38ddf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d36bdd248142d780bfbb7bcf169bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/522M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Loading PEFT model\n",
    "PEFT_MODEL = \"omarfarooq908/falcon-7b-finetuned01\"\n",
    "# PEFT_MODEL = <Username>/YOUR_MODEL_URL_REPO. \n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "peft_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_base_model, PEFT_MODEL)\n",
    "\n",
    "peft_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "peft_tokenizer.pad_token = peft_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "652a9962-3d9f-42f7-a26a-33e96119d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "def generate_answer(query):\n",
    "  system_prompt = \"\"\"Answer the following question truthfully.\n",
    "  You are a chatbot that can coherently hold conversations with humans, in English\"\"\"\n",
    "\n",
    "  user_prompt = f\"\"\": {query}\n",
    "  : \"\"\"\n",
    "\n",
    "  final_prompt = system_prompt + \"\\n\" + user_prompt\n",
    "\n",
    "  device = \"cuda:0\"\n",
    "  dashline = \"-\".join(\"\" for i in range(50))\n",
    "\n",
    "  encoding = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "  outputs = model.generate(input_ids=encoding.input_ids, generation_config=GenerationConfig(max_new_tokens=256, pad_token_id = tokenizer.eos_token_id, \\\n",
    "                                                                                                                     eos_token_id = tokenizer.eos_token_id, attention_mask = encoding.attention_mask, \\\n",
    "                                                                                                                     temperature=0.4, top_p=0.6, repetition_penalty=1.3, num_return_sequences=1,))\n",
    "  text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "  print(dashline)\n",
    "  print(f'ORIGINAL MODEL RESPONSE:\\n{text_output}')\n",
    "  print(dashline)\n",
    "\n",
    "  peft_encoding = peft_tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "  peft_outputs = peft_model.generate(input_ids=peft_encoding.input_ids, generation_config=GenerationConfig(max_new_tokens=256, pad_token_id = peft_tokenizer.eos_token_id, \\\n",
    "                                                                                                                     eos_token_id = peft_tokenizer.eos_token_id, attention_mask = peft_encoding.attention_mask, \\\n",
    "                                                                                                                     temperature=0.4, top_p=0.6, repetition_penalty=1.3, num_return_sequences=1,))\n",
    "  peft_text_output = peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "  print(f'PEFT MODEL RESPONSE:\\n{peft_text_output}')\n",
    "  print(dashline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01eb8a2b-5c0d-4ac1-bae0-718595f3b6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Answer the following question truthfully.\n",
      "  You are a chatbot that can coherently hold conversations with humans, in English\n",
      ": Tell me a story\n",
      "  : (I'm not sure what you mean by \"story\", but I'll try to tell you a story)\n",
      "  : (I'm not sure what you mean by \"story\", but I'll try to tell you a story)\n",
      "\n",
      "The chatbot is able to tell a story, but it is not very good at it.\n",
      "\n",
      "The chatbot is able to tell a story, but it is not very good at it.\n",
      "\n",
      "The chatbot is able to tell a story, but it is not very good at it.\n",
      "\n",
      "The chatbot is able to tell a story, but it is not very good at it.\n",
      "\n",
      "The chatbot is able to tell a story, but it is not very good at it.\n",
      "\n",
      "The chatbot is able to tell a story, but it is not very good at it.\n",
      "\n",
      "The chatbot is able to tell a story, but it is not very good at it.\n",
      "\n",
      "The chatbot is able to tell a story, but it is not very good at it.\n",
      "\n",
      "The chatbot is able to tell a story, but it is not very good at it.\n",
      "\n",
      "The chatbot is able to tell a story, but it\n",
      "-------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Answer the following question truthfully.\n",
      "  You are a chatbot that can coherently hold conversations with humans, in English\n",
      ": Tell me a story\n",
      "  : \"Once upon a time, there was a princess who lived in a castle. She had a pet cat named Fluffy. One day, the princess went to the market to buy some food for her cat. While she was shopping, she met a handsome prince who asked her out on a date. The princess agreed and they went to a fancy restaurant where they ate delicious food and drank wine. After dinner, the princess and the prince went back to the castle where they danced and kissed all night long. The next morning, the princess woke up and found that Fluffy had eaten all of the food in the kitchen. She was so angry that she threw Fluffy out of the castle. The princess never saw the prince again.\"\n",
      "\n",
      "What is the probability that the chatbot will be able to answer this question truthfully?\n",
      "\n",
      "What is the probability that the chatbot will be able to answer this question truthfully?\n",
      "\n",
      "What is the probability that the chatbot will be able to answer this question truthfully?\n",
      "\n",
      "What is the probability that the chatbot will be able to answer this question truthfully?\n",
      "\n",
      "What is the probability that the chatbot will be able to answer this question truthfully?\n",
      "\n",
      "What is the probability that the chatbot\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\"Tell me a story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2adc97-ae16-4211-b301-9aada0aafc42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-test002-02",
   "language": "python",
   "name": "myenv-test002-02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
